## 个人简历

### 基本信息

严世伟｜男｜18559652882｜372607703@qq.com｜求职意向：数据开发｜期望薪资：16-18K｜期望城市：厦门

### 个人优势

1. 拥有13年的Java编程经验，以及6年的大数据领域工作经验； 
2. 熟练掌握Java和Scala语言编程，熟悉多种设计模式并能运用在代码的设计重构中，原码阅读能力强能，能快速上手工作； 
3. 熟悉多种数据存储数据库，包括ElasticSearch、MongoDB、Doris、PostgreSql、MySql、JanusGraph(Gremlin)、NebulaGraph、Redis、HBase等，以及消息中间件Kafka的使用； 
4. 同时具备实时计算(Storm/Spark/Flink)和离线计算(Hive/Spark/Hadoop)等常用大数据解决方案的实践经验。

### 工作经历

#### 2018.8-至今｜美亚柏科信息科技股份有限公司｜数据开发

1. 功能设计：负责数据仓库的设计、开发和维护，保证数据的准确性和完整性； 
2. 数据处理：负责数据数据处理、数据清洗、数据存储、数据分析等数据开发工作； 
3. 数据分析：负责数据挖掘和分析，为业务提供数据支持和决策依据； 
4. 数据运维：负责大数据平台的性能优化和故障排查，保证系统的稳定性和可靠性； 
5. 标准制定：参与大数据标准制定，验证评分其他公司标准使用情况； 
6. 技术培训：负责技前沿技术调研选型，整理技术文档专利，培训分享技术；
7. 技术对接：负责收集整理友商需求，沟通功能接口文档或提供用户指南。

#### 2015.5-2018.8 | 厦门美聚科技有限公司 | JAVA

1. 核心编码：负责公司产品核心业务代码编写； 
2. 方案编写：负责协助架构师进行技术方案的编写、编码； 
3. 技术预研：负责技术方案中需要用到的第三方技术调研，并推动技术方案的实现； 
4. 方案沟通：负责协调开发与产品之间的沟通，完成产品需求的最终实现； 
5. 平台搭建：负责配合系统架构师，进行平台架构体系的搭建以及优化事项。

#### 2011.7-2015.5 | 厦门中软海晟信息技术有限公司 | JAVA

1. 接口编写：负责后端微服务接口编写、数据统计与分析； 
2. 数据统计：负责数据开发工作，包括离线统计、实时计算； 
3. 任务分配：担任开发小组长，负责开发核心功能，拆分分配工作，解决复杂问题。

### 项目经历

#### 星火大数据平台解析服务

* 项目架构：Kafka+ElasticSearch+PostgreSql+Doris+MySql+SpringBoot
* 项目描述：数据接入服务是一种帮助企业和组织将多来源、多格式的数据高效接入到其数据平台的解决方案。通过标准化、自动化的工具，数据接入服务能够快速采集、转换和整合各类内部或外部的数据源，确保数据的高效传输与准确性。这不仅提升了数据的可用性，还为后续的数据分析、业务决策提供了可靠的基础，助力企业实现智能化运营和数字化转型。
* 责任描述：
  1. **支持多种数据标准**。目前支持BCP、EFP、XML等数据标准的解析。
  2. **上下文**。为每个数据包解析创建单独的解析上下文，并对上下文进行生命周期管理。
  3. ***数据校验***。对数据包的包名、md5、fastMd5等进行校验，防止数据重复、数据篡改。
  4. ***数据映射***。根据配置字段映射、字典映射。
  5. **数据类型处理**。对字符串、数字、时间等进行解析和处理。
  6. ***归一化处理***。对手机号、mac地址、ip地址等进行格式归一化。
  7. **公共字段处理**。根据业务需要冗余，包括数据包id、排重id、人员id等
  8. **表拆分、合并处理**。为了兼容多种数据标准的库表结构，支持一张表拆分成多张表，多张表合并成一张表。
  9. **日志和进度**。解析过程记录日志和进度，供可视化展示。
  10. **指标监控**。对每张表的处理情况和完成情况进行记录，用于推动整体流程的流转进行。
  11. **状态管理**。控制解析启动、停止、重新解析等。
  12. **异常处理**。梳理常见异常类型，方便问题排查和页面展示。
  13. **分布式解析**。支持多个解析任务同时执行，并支持动态扩容、缩容。
  14. **数据一致性**。通过状态记录、重试和幂等性，保证数据处理的“精确一次性”。

#### 星火大数据平台整体数据治理架构

* 项目架构：解析服务+消息队列+流式计算+离线计算+监控+数仓
* 项目描述：数据治理架构通过解析服务整合多源数据，利用消息队列实现数据实时传输，结合流式计算进行数据实时处理，离线计算则负责大规模历史数据的分析。监控模块保障数据处理的可视化与实时监控，数据仓库（数仓）则为数据存储、管理和分析提供基础支持，确保数据的高效治理与业务决策优化。
* 责任描述：
  1. ***解析服务***。整合多源数据，解析数据并将数据处理为统一格式后分发。
  2. ***消息队列***。解耦合 ，解析好的数据可以入一份到ODS层，同时可以由后续的处理服务对其进行进一步的处理。
  3. ***流式计算***。将消息队列中的数据进一步进行简单处理，包括关键词提取、日期提取、亲属关系提取等，作为DWD层。
  4. ***离线计算***。针对一批数据，根据数据使用场景不同，使用离线计算将数据再统计、挖掘成业务需要展示格式，作为DWS层。
  5. ***监控***。根据流计算、离线计算的子任务进行监控，自动推动下一阶段任务的执行。
  6. ***数仓***。根据数据的使用频度，将一些常用的表进行建模，提高设计利用率，提高数据查询效率。

#### 分布式文件存储系统

* 项目架构：Hadoop+HDFS+HBase+Kafka+MongoDB+Zookeeper
* 项目描述: 随着终端设备存储容量的不断提升，数据包中获得各类多媒体文件（图片、音频、视频等）越来越多，使用 HDFS、HBase、Minio  这类开源文件存储需要逐个上传海量大附件容易积压，上传海量小附件是容易出现分布式存储的小文件问题，而且出错时难以排查和解决问题，并且缺少对各节点状态的监控，也缺少所有附件关联业务数据的管理。综上所述，本项目提供了一站式的解决方案，在方便扩容的同时，也满足附件存储速度的要求，并定制化加入对业务数据的支持。
* 责任描述：
  1. 参与项目的前期架构设计；
  2. 数据包由前置服务生成数据包记录存储入 MongoDB ，并根据文件大小分别存储到 HDFS  或者 HBase上； 
  3. 使用相同消费组的 Kafka  客户端构成分布式多线程的架构； 
  4. 节点分别在在各自的约定的目录中解压分解数据包，使用 zookeeper  做为分布式锁控制；
  5.  节点再将解压后将节点信息和路径信息存储到表中，减少了上传附件的过程，从而避免了存储到第三方服务带来的积压问题； 
  6.  同时基于附件表中的记录，可以实现恢复与迁移的功能；
  7.  扩容时需要增加 Kafka  的分区，然后增加分布式节点即可。

#### 统计区块链交易数据

* 项目架构：SparkSQL+Hive+Hadoop+HDFS+MongoDB+NebulaGraph+SpringBoot+Kafka+Zookeeper
* 项目描述：随着数字货币在互联网上的流行，加之电信诈骗的兴起，分析交易行为、交易金额、交易频率、资金流向等就尤为 重要。本项目通过互联网获取 ETH、TRON、BSC、BTC  数据保存入 Mongo  作为原始数据。将一份数据入库 Hive  做为统计交易数量使用。将另一份数据存储入 NebulaGraph  中作为关系分析使用。
* 责任描述：
  1. 参与项目的前期架构设计；
  2. 以日期为单位将统计任务表中，数据量完整的数据导入到 HDFS中，使用 Hive  外部分区表存储一份交易数据； 
  3. 并将 Hive  存储完成日期分区表的日期发送给 kafka ，可分布式多线程同时计算多个日期的数据，加快统计速度；
  4. 使用 SparkSQL  统计每日的数据，并存储到 MongoDB  的日统计表中，并将每日数据累加生成总量数据存储到总量表中； 
  5. 将发送和接收交易地址在 NebulaGraph  的顶点中查找是否存在，不存在则新增； 
  6. 更新发送地址和接收地址的总量边和日期边的交易金额、交易笔数、交易手续费等； 
  7. 交易完成后修改任务表状态完整一天的统计，从而实现实时自动化统计区块链交易数据。

### 教育经历

#### 2007.9-2011.7｜仰恩大学｜计算机科学与技术｜本科｜学士



