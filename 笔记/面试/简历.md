## 个人简历

### 基本信息

严世伟｜男｜18559652882｜372607703@qq.com｜求职意向：数据开发｜期望薪资：18-23K｜期望城市：厦门

### 个人优势

1. 拥有13年的Java编程经验，以及6年的大数据领域工作经验； 
2. 熟练掌握Java和Scala语言编程，熟悉多种设计模式并能运用在代码的设计重构中，原码阅读能力强能，能快速上手工作； 
3. 熟悉多种数据存储数据库，包括ElasticSearch、MongoDB、Doris、PostgreSql、MySql、JanusGraph(Gremlin)、NebulaGraph、Redis、HBase等，以及消息中间件Kafka的使用； 
4. 同时具备实时计算(Storm/Spark)和离线计算(Hive/Spark/Hadoop/Yarn)等常用大数据解决方案的实践经验。

### 工作经历

#### 2018.8-至今｜美亚柏科信息科技股份有限公司｜数据开发

1. 功能设计：负责数据仓库的设计、开发和维护，保证数据的准确性和完整性； 
2. 数据处理：负责数据数据处理、数据清洗、数据存储、数据分析等数据开发工作； 
3. 数据分析：负责数据挖掘和分析，为业务提供数据支持和决策依据； 
4. 数据运维：负责大数据平台的性能优化和故障排查，保证系统的稳定性和可靠性； 
5. 标准制定：参与大数据标准制定，验证评分其他公司标准使用情况； 
6. 技术培训：负责技前沿技术调研选型，整理技术文档专利，培训分享技术；
7. 技术对接：负责收集整理友商需求，沟通功能接口文档或提供用户指南。

#### 2015.5-2018.8 | 厦门美聚科技有限公司 | JAVA

1. 核心编码：负责公司产品核心业务代码编写； 
2. 方案编写：负责协助架构师进行技术方案的编写、编码； 
3. 技术预研：负责技术方案中需要用到的第三方技术调研，并推动技术方案的实现； 
4. 方案沟通：负责协调开发与产品之间的沟通，完成产品需求的最终实现； 
5. 平台搭建：负责配合系统架构师，进行平台架构体系的搭建以及优化事项。

#### 2011.7-2015.5 | 厦门中软海晟信息技术有限公司 | JAVA

1. 接口编写：负责后端微服务接口编写、数据统计与分析； 
2. 数据统计：负责数据开发工作，包括离线统计、实时计算； 
3. 任务分配：担任开发小组长，负责开发核心功能，拆分分配工作，解决复杂问题。

### 项目经历

#### 电子信息研判数据平台

* 项目架构：Storm+Hadoop+HBase+HDFS+Kafka+ElasticSearch+MongoDB+MySql+Titan+SpringBoot+Vue
* 项目描述：随着信息化的开展，需要将结构化数据与非结构化数据汇聚与分析多种格式的数据。项目将某行业不同标准的结构化数据进行解析和预处理、根据业务需要将数据存储到不同存储组件当中，并对数据进行统计、关联碰撞、打标等，为上层业务展示提供数据支持。并将平台分析出的结果数据根据需求格式重新生成结构化数据，供各级单位汇聚使用。
* 责任描述：
  1. ***数据校验***。根据多种数据标准的结构化数据进行校验、排重、解析； 
  2. ***数据转换***。根据配置在数据库中字段级别的映射对数据进行转换，将多种数据格式映射成统一套库表；
  3. ***统一格式***。对数据归一化、格式处理、字段类型转换等一系列处理；
  4. ***特征选择和提取***。选择最相关的特征或者从原始数据中提取新的特征，以用于建模。
  5. ***持久化结构化数据***。将结构化数据根据业务需要存储到 ElasticSearch，作为DWD层； 
  6. ***持久化非结构化数据***。将非结构化数据如：图片、音频、视频等存储到 HDFS、HBase中； 
  7. ***批次预统计***。针对一批数据，为应对数据量大时的查询与统计，设计多种模型对数据进行小范围预统计，作为DWS层。
  8. ***深度数据分析***。根据数据使用场景不同，使用离线计算将数据再统计、挖掘成业务需要展示格式，作为ADS。
  9. ***数据分发***。根据需求格式不同，将同一套数据映射、重新打包，并分发到不同的目的端。

#### 分布式文件存储系统

* 项目架构：Hadoop+HDFS+HBase+Kafka+MongoDB+Zookeeper
* 项目描述: 随着终端设备存储容量的不断提升，数据包中获得各类多媒体文件（图片、音频、视频等）越来越多，使用 HDFS、HBase、Minio  这类开源文件存储需要逐个上传海量大附件容易积压，上传海量小附件是容易出现分布式存储的小文件问题，而且出错时难以排查和解决问题，并且缺少对各节点状态的监控，也缺少所有附件关联业务数据的管理。综上所述，本项目提供了一站式的解决方案，在方便扩容的同时，也满足附件存储速度的要求，并定制化加入对业务数据的支持。
* 责任描述：
  1. 参与项目的前期架构设计；
  2. 数据包由前置服务生成数据包记录存储入 MongoDB ，并根据文件大小分别存储到 HDFS  或者 HBase上； 
  3. 使用相同消费组的 Kafka  客户端构成分布式多线程的架构； 
  4. 节点分别在在各自的约定的目录中解压分解数据包，使用 zookeeper  做为分布式锁控制；
  5.  节点再将解压后将节点信息和路径信息存储到表中，减少了上传附件的过程，从而避免了存储到第三方服务带来的积压问题； 
  6.  同时基于附件表中的记录，可以实现恢复与迁移的功能；
  7.  扩容时需要增加 Kafka  的分区，然后增加分布式节点即可。

#### 统计区块链交易数据

* 项目架构：SparkSQL+Hive+Hadoop+HDFS+MongoDB+NebulaGraph+SpringBoot+Kafka+Zookeeper
* 项目描述：随着数字货币在互联网上的流行，加之电信诈骗的兴起，分析交易行为、交易金额、交易频率、资金流向等就尤为 重要。本项目通过互联网获取 ETH、TRON、BSC、BTC  数据保存入 Mongo  作为原始数据。将一份数据入库 Hive  做为统计交易数量使用。将另一份数据存储入 NebulaGraph  中作为关系分析使用。
* 责任描述：
  1. 参与项目的前期架构设计；
  2. 以日期为单位将统计任务表中，数据量完整的数据导入到 HDFS中，使用 Hive  外部分区表存储一份交易数据； 
  3. 并将 Hive  存储完成日期分区表的日期发送给 kafka ，可分布式多线程同时计算多个日期的数据，加快统计速度；
  4. 使用 SparkSQL  统计每日的数据，并存储到 MongoDB  的日统计表中，并将每日数据累加生成总量数据存储到总量表中； 
  5. 将发送和接收交易地址在 NebulaGraph  的顶点中查找是否存在，不存在则新增； 
  6. 更新发送地址和接收地址的总量边和日期边的交易金额、交易笔数、交易手续费等； 
  7. 交易完成后修改任务表状态完整一天的统计，从而实现实时自动化统计区块链交易数据。

### 教育经历

#### 2007.9-2011.7｜仰恩大学｜计算机科学与技术｜本科｜学士



