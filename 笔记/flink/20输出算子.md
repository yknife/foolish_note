## 输出算子

### 输出到文件

```java
public class SinkFile {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 每个目录中，都有 并行度个数的 文件在写入
        env.setParallelism(2);

        // 必须开启checkpoint，否则一直都是 .inprogress
        // 注意：在mac中运行时
        //			1.观察并没有生成.inprogress文件（一开始不生成文件，以为是权限问题）
      	//			2.DefaultRollingPolicy.withMaxPartSize(new MemorySize(1024),将滚动文件大小改为1k后，能立即生成文件，看到效果
        env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);


        DataGeneratorSource<String> dataGeneratorSource = new DataGeneratorSource<>(
                new GeneratorFunction<Long, String>() {
                    @Override
                    public String map(Long value) throws Exception {
                        return "Number:" + value;
                    }
                },
                Long.MAX_VALUE,
                RateLimiterStrategy.perSecond(1000),
                Types.STRING
        );

        DataStreamSource<String> dataGen = env.fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), "data-generator");

        // 输出到文件系统
        FileSink<String> fieSink = FileSink
                // 输出行式存储的文件，指定路径、指定编码
                .<String>forRowFormat(new Path("f:/tmp"), new SimpleStringEncoder<>("UTF-8"))
                // 输出文件的一些配置： 文件名的前缀、后缀
                .withOutputFileConfig(
                        OutputFileConfig.builder()
                                .withPartPrefix("atguigu-")
                                .withPartSuffix(".log")
                                .build()
                )
                // 按照目录分桶：如下，就是每个小时一个目录
                .withBucketAssigner(new DateTimeBucketAssigner<>("yyyy-MM-dd HH", ZoneId.systemDefault()))
                // 文件滚动策略:  1分钟 或 1m
                .withRollingPolicy(
                        DefaultRollingPolicy.builder()
                                .withRolloverInterval(Duration.ofMinutes(1))
                                .withMaxPartSize(new MemorySize(1024*1024))
                                .build()
                )
                .build();


        dataGen.sinkTo(fieSink);

        env.execute();
    }
}
```

### 输出到Kafka

1. 添加Kafka 连接器依赖

2. 启动Kafka集群

3. 编写输出到Kafka的示例代码

   输出无key的record

   ```java
   public class SinkKafka {
       public static void main(String[] args) throws Exception {
           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
           env.setParallelism(1);
           // 如果是精准一次，必须开启checkpoint（后续章节介绍）
           env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);
           SingleOutputStreamOperator<String> sensorDS = env
                   .socketTextStream("hadoop102", 7777);
   
           /**
            * Kafka Sink:
            * TODO 注意：如果要使用 精准一次 写入Kafka，需要满足以下条件，缺一不可
            * 1、开启checkpoint（后续介绍）
            * 2、设置事务前缀
            * 3、设置事务超时时间：   checkpoint间隔 <  事务超时时间  < max的15分钟
            */
           KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                   // 指定 kafka 的地址和端口
                   .setBootstrapServers("hadoop102:9092,hadoop103:9092,hadoop104:9092")
                   // 指定序列化器：指定Topic名称、具体的序列化
                   .setRecordSerializer(
                           KafkaRecordSerializationSchema.<String>builder()
                                   .setTopic("ws")
                                   .setValueSerializationSchema(new SimpleStringSchema())
                                   .build()
                   )
                   // 写到kafka的一致性级别： 精准一次、至少一次
                   .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                   // 如果是精准一次，必须设置 事务的前缀
                   .setTransactionalIdPrefix("atguigu-")
                   // 如果是精准一次，必须设置 事务超时时间: 大于checkpoint间隔，小于 max 15分钟
                   .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10*60*1000+"")
                   .build();
           sensorDS.sinkTo(kafkaSink);
           env.execute();
       }
   }
   ```

   输出有key的record

   ```java
   public class KafkaSinkWithKeyExample {
   
       public static void main(String[] args) throws Exception {
   
           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
           env.setParallelism(1);
           env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);
           DataStreamSource<String> source = env.socketTextStream("localhost", 8888);
   
           KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                   .setBootstrapServers("localhost:9092")
                   .setRecordSerializer(
                           new KafkaRecordSerializationSchema<String>(){
                               @Nullable
                               @Override
                               public ProducerRecord<byte[], byte[]> serialize(String element, KafkaSinkContext context, Long timestamp) {
                                   String[] split = element.split(",");
                                   byte[] key = split[0].getBytes(StandardCharsets.UTF_8);
                                   return new ProducerRecord<>("test_topic",key,element.getBytes(StandardCharsets.UTF_8));
                               }
                           }
                   )
                   .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                   .setTransactionalIdPrefix("yknife-")
                   .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10 * 60 * 1000 + "")
                   .build();
   
           source.sinkTo(kafkaSink);
           env.execute();
       }
   }
   ```

   ### 输出到JDBC

   1. 添加依赖

      ```groovy
      compile 'org.apache.flink:flink-connector-jdbc:3.1.1-1.17'
      compile 'mysql:mysql-connector-java:5.1.49'
      ```

   2. 创建表

      ```sql
      CREATE TABLE `ws` (
        `id` varchar(100) NOT NULL,
        `ts` bigint(20) DEFAULT NULL,
        `vc` int(11) DEFAULT NULL,
        PRIMARY KEY (`id`)
      ) ENGINE=InnoDB DEFAULT CHARSET=utf8
      ```

   3. 编写代码

      ```java
      public class SinkJdbcExample {
      
          public static void main(String[] args) throws Exception {
              StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
              env.setParallelism(1);
              DataStreamSource<String> source = env.socketTextStream("localhost", 8888);
              SingleOutputStreamOperator<WaterSensor> mapDs = source.map(new WaterSensorMapFunction());
              SinkFunction<WaterSensor> jdbcSink = JdbcSink.sink(
                      "insert into ws values(?,?,?)",
                      new JdbcStatementBuilder<WaterSensor>() {
                          @Override
                          public void accept(PreparedStatement preparedStatement, WaterSensor waterSensor) throws SQLException {
                              preparedStatement.setString(1, waterSensor.getId());
                              preparedStatement.setLong(2, waterSensor.getTs());
                              preparedStatement.setInt(3, waterSensor.getVc());
                          }
                      },
                      JdbcExecutionOptions.builder()
                              .withMaxRetries(3) // 重试次数
                              .withBatchSize(100) // 批次的大小：条数
                              .withBatchIntervalMs(3000) // 批次的时间
                              .build(),
                      new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                              .withUrl("jdbc:mysql://localhost:3306/spark?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=UTF-8")
                              .withUsername("root")
                              .withPassword("1")
                              .withConnectionCheckTimeoutSeconds(60) // 重试的超时时间
                              .build()
              );
              mapDs.addSink(jdbcSink);
              env.execute();
          }
      }
      ```

      ### 自定义Sink

      如果我们想将数据存储到我们自己的存储设备中，而Flink并没有提供可以直接使用的连接器，就只能自定义Sink进行输出了。与Source类似，Flink为我们提供了通用的SinkFunction接口和对应的RichSinkFunction抽象类，只要实现它，通过简单地调用DataStream的.addSink()方法就可以自定义写入任何外部存储。

      ```java
      stream.addSink(new MySinkFunction<String>());
      ```

      在实现SinkFunction的时候，需要重写的一个关键方法invoke()，在这个方法中我们就可以实现将流里的数据发送出去的逻辑。

      这种方式比较通用，对于任何外部存储系统都有效；不过自定义Sink想要实现状态一致性并不容易，所以一般只在没有其它选择时使用。实际项目中用到的外部连接器Flink官方基本都已实现，而且在不断地扩充，因此自定义的场景并不常见。

      注意：创建消费连接时，使用RichSinkFunction的open、close方法，分别只会在环境创建和销毁时调用一次。

   
