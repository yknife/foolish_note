##  RDD 创建

在 Spark 中创建 RDD 的创建方式可以分为四种

###    从集合中创建RDD

```Scala
val seq = Seq[Int](1,2,3,4)



// 第二个参数为数据切片/分区的数量，第二个参数不给时，使用默认值defaultParallelism（默认并行度）



//   spark在默认情况下，从配置对象中获取配置参数：spark.default.parallelism



//   如果获取不到，那么使用totalCores属性，这个属性取值为当前运行环境的最大可用核数



val rdd1: RDD[Int] = sparkContext.parallelize(seq, 2)



// makeRDD方法在底层实现时其实就是调用了rdd对象的parallelize方法



val rdd2: RDD[Int] = sparkContext.makeRDD(seq, 2)



 



// 集合读取数据时，每个切片/分区分配的数据地址为



def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {



  (0 until numSlices).iterator.map { i =>



    val start = ((i * length) / numSlices).toInt



    val end = (((i + 1) * length) / numSlices).toInt



    (start, end)



  }



}
```

###   从文件中创建RDD

```Scala
// 可以是具体文件，第二个参数为数据切片/分区的最小分区数量，第二个参数不给时，使用默认值math.min(defaultParallelism, 2)



val rdd1: RDD[String] = sparkContext.textFile("datas/1.txt", 3)



// 可以是目录



val rdd2: RDD[String] = sparkContext.textFile("datas")



// 可以使用通配符 *



val rdd3: RDD[String] = sparkContext.textFile("datas/1*.txt")



// 可以是分布式存储系统路径：HDFS



val rdd4: RDD[String] = sparkContext.textFile("hdfs://linux1:8020/test.txt")



// 以文件为单位读取数据，读取的结果表示为元组，第一个元素表示文件路径，第二个元素表示文件内容



val rdd5: RDD[(String, String)] = sparkContext.wholeTextFiles("datas")
```

​    读取文件数据时，数据是按照 Hadoop 文件读取的规则进行切片分区
   文件读取数据时实际切片/分区数 >=最小分区数
​    单个文件时
​    分区数的计算方式：
​      每个分区字节数 = 文件总字节数 / 最小分区数
​      分区数 ≈ 文件总字节数 / 每个分区字节数
​       余数部分>每个分区字节数*0.1时，分区数+1
​    数据分区的划分：
​      1.数据以行为单位进行读取，一行数据只会属于一个分区
​      2.数据分区偏移量计算
​       例如：文件总字节数为7，最小分区数设为2，每个分区的字节数为3，实际分区数为3
​        分区1的偏移量： [0, 3]
​        分区2的偏移量： [3, 6]
​        分区3的偏移量： [7]
​      3.以每行数据的第一个字节的偏移量来决定属于哪个分区
​    如果数据源为多个文件，那么计算分区时以文件为单位进行分区