## RDD 序列化

### 闭包检测

​	从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor 端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就 形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列 化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变。

### 序列化方法和属性

从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor 端执行，看如下代码：

```scala
object serializable02_function {
 def main(args: Array[String]): Unit = {
 //1.创建 SparkConf 并设置 App 名称
 val conf: SparkConf = new
SparkConf().setAppName("SparkCoreTest").setMaster("local[*]")
 //2.创建 SparkContext，该对象是提交 Spark App 的入口
 val sc: SparkContext = new SparkContext(conf)
 //3.创建一个 RDD
 val rdd: RDD[String] = sc.makeRDD(Array("hello world", "hello spark",
"hive", "atguigu"))
 //3.1 创建一个 Search 对象
 val search = new Search("hello")
 //3.2 函数传递，打印：ERROR Task not serializable
 search.getMatch1(rdd).collect().foreach(println)
 //3.3 属性传递，打印：ERROR Task not serializable
 search.getMatch2(rdd).collect().foreach(println)
 //4.关闭连接
 sc.stop()
 }
}
//也可使case class class Search(query:String){}
class Search(query:String) extends Serializable {
 def isMatch(s: String): Boolean = {
 s.contains(query)
 }
 // 函数序列化案例
 def getMatch1 (rdd: RDD[String]): RDD[String] = {
 //rdd.filter(this.isMatch)
 rdd.filter(isMatch)
 }
  // 属性序列化案例
 def getMatch2(rdd: RDD[String]): RDD[String] = {
 //rdd.filter(x => x.contains(this.query))
 rdd.filter(x => x.contains(query))
 //val q = query
 //rdd.filter(x => x.contains(q))
 }
}
```

### Kryo 序列化框架

​	参考地址: https://github.com/EsotericSoftware/kryo 

​	Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也 比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度 是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型 已经在 Spark 内部使用 Kryo 来序列化。 

注意：即使使用 Kryo 序列化，也要继承 Serializable 接口。

```scala
object serializable_Kryo {
 def main(args: Array[String]): Unit = {
 val conf: SparkConf = new SparkConf()
 .setAppName("SerDemo")
 .setMaster("local[*]")
 // 替换默认的序列化机制
 .set("spark.serializer",
"org.apache.spark.serializer.KryoSerializer")
 // 注册需要使用 kryo 序列化的自定义类
 .registerKryoClasses(Array(classOf[Searcher]))
 val sc = new SparkContext(conf)
 val rdd: RDD[String] = sc.makeRDD(Array("hello world", "hello atguigu",
"atguigu", "hahah"), 2)
 val searcher = new Searcher("hello")
 val result: RDD[String] = searcher.getMatchedRDD1(rdd)
 result.collect.foreach(println)
 }
}
case class Searcher(val query: String) {
 def isMatch(s: String) = {
 s.contains(query)
 }
 def getMatchedRDD1(rdd: RDD[String]) = {
 rdd.filter(isMatch)
 }
 def getMatchedRDD2(rdd: RDD[String]) = {
 val q = query
 rdd.filter(_.contains(q))
 }
}
```

