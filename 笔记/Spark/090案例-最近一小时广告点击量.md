```scala
package com.example.streaming

import java.io.{File, FileWriter, PrintWriter}
import java.text.SimpleDateFormat
import java.util.Date

import com.example.spark.util.JdbcUtil
import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord}
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.collection.mutable.ListBuffer

object CoreExample3 {
  def main(args: Array[String]): Unit = {
    val sparkConf = new
        SparkConf().setMaster("local[*]").setAppName("KafkaSource")
    val ssc = new StreamingContext(sparkConf, Seconds(5))
    val kafkaPara: Map[String, Object] = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG ->
        "localhost:9092",
      ConsumerConfig.GROUP_ID_CONFIG -> "test_group",
      "key.deserializer" ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      "value.deserializer" ->
        "org.apache.kafka.common.serialization.StringDeserializer"
    )

    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](Set("test_topic"), kafkaPara)
    )
    val ds: DStream[AdClickData] = kafkaDStream.map(
      row => {
        val line: String = row.value()
        val arr = line.split(" ")
        val adClickData = AdClickData(arr(0), arr(1), arr(2), arr(3), arr(4))
        adClickData
      }
    )

    val reduceDs: DStream[(Long, Int)] = ds.map(
      data => {
        val ts = data.ts.toLong
        val newTs = ts / 10000 * 10000
        (newTs, 1)
      }
    ).reduceByKeyAndWindow((x:Int,y:Int)=>x+y, Seconds(60), Seconds(10))

    reduceDs.foreachRDD(
      rdd=>{
        val listBuffer = ListBuffer[String]()

        val tuples: Array[(Long, Int)] = rdd.sortByKey(true).collect()
        tuples.foreach{
          case(ts,count)=>{
            val str = new SimpleDateFormat("mm:ss").format(new Date(ts))
            listBuffer.append(s"""{ "xtime":"$str", "yval":"$count" }""")
          }
        }

        val output = new PrintWriter(new FileWriter(new File("/Users/yknife/IdeaProjects/spark/datas/adclick/adclick.json")))
        output.println("["+listBuffer.mkString(",")+"]")
        output.flush()
        output.close()
      }

    )


    ssc.start()
    ssc.awaitTermination()
  }
  case class AdClickData(ts:String,area:String,city:String,user:String,ad:String)
}
```

```scala
package com.example.streaming

import java.util.concurrent.TimeUnit
import java.util.{Properties, Random}

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}

import scala.collection.mutable.ListBuffer

object MockData {

  // 时间戳 区域 城市 用户 广告
  def mock():ListBuffer[String] = {
    val list: ListBuffer[String] = ListBuffer[String]()
    val areaList = ListBuffer[String]("华北", "华东", "华南")
    val cityList = ListBuffer[String]("北京", "上海", "深圳")
    for(i <- 1 to new Random().nextInt(50)) {
      val area: String = areaList(new Random().nextInt(areaList.size))
      val city = cityList(new Random().nextInt(cityList.size))
      val userId = new Random().nextInt(6) + 1
      val adId = new Random().nextInt(6) + 1
      list.append(s"${System.currentTimeMillis()} ${area} ${city} ${userId} ${adId}")
    }
    list
  }
  def main(args: Array[String]): Unit = {

    // 创建配置对象
    val prop = new Properties()
    // 添加配置
    prop.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
    prop.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    prop.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    // 根据配置创建 Kafka 生产者
    val producer = new KafkaProducer[String, String](prop)
    while (true){
      mock().foreach(
        r=>{
          val record = new ProducerRecord[String,String]("test_topic", r)
          producer.send(record)
        }
      )
      TimeUnit.SECONDS.sleep(1)
    }
  }
}
```