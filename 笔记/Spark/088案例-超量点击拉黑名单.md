### 生成数据到kafka

```scala
package com.example.streaming

import java.util.concurrent.TimeUnit
import java.util.{Properties, Random}

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}

import scala.collection.mutable.ListBuffer

object MockData {

  // 时间戳 区域 城市 用户 广告
  def mock():ListBuffer[String] = {
    val list: ListBuffer[String] = ListBuffer[String]()
    val areaList = ListBuffer[String]("华北", "华东", "华南")
    val cityList = ListBuffer[String]("北京", "上海", "深圳")
    for(i <- 1 to 10) {
      val area: String = areaList(new Random().nextInt(areaList.size))
      val city = cityList(new Random().nextInt(cityList.size))
      val userId = new Random().nextInt(6) + 1
      val adId = new Random().nextInt(6) + 1
      list.append(s"${System.currentTimeMillis()} ${area} ${city} ${userId} ${adId}")
    }
    list
  }
  def main(args: Array[String]): Unit = {

    // 创建配置对象
    val prop = new Properties()
    // 添加配置
    prop.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
    prop.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    prop.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    // 根据配置创建 Kafka 生产者
    val producer = new KafkaProducer[String, String](prop)
    while (true){
      mock().foreach(
        r=>{
          val record = new ProducerRecord[String,String]("test_topic", r)
          producer.send(record)
        }
      )
      TimeUnit.SECONDS.sleep(1)
    }
  }
}
```

### 计算点击量，超限后拉入黑名单

```scala
package com.example.streaming

import java.text.SimpleDateFormat
import java.util.Date

import com.example.spark.util.JdbcUtil
import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord}
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.collection.mutable.ListBuffer

object CoreExample1 {
  def main(args: Array[String]): Unit = {
    val sparkConf = new
        SparkConf().setMaster("local[*]").setAppName("KafkaSource")
    val ssc = new StreamingContext(sparkConf, Seconds(3))
    val kafkaPara: Map[String, Object] = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG ->
        "localhost:9092",
      ConsumerConfig.GROUP_ID_CONFIG -> "test_group",
      "key.deserializer" ->
        "org.apache.kafka.common.serialization.StringDeserializer",
      "value.deserializer" ->
        "org.apache.kafka.common.serialization.StringDeserializer"
    )

    val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](
      ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](Set("test_topic"), kafkaPara)
    )
    val ds: DStream[AdClickData] = kafkaDStream.map(
      row => {
        val line: String = row.value()
        val arr = line.split(" ")
        val adClickData = AdClickData(arr(0), arr(1), arr(2), arr(3), arr(4))
        adClickData
      }
    )

    val wcDs: DStream[((String, String, String), Int)] = ds.transform(
      rdd => {
        val blackList = ListBuffer[String]()
        val connection = JdbcUtil.getConnection
        val statement = connection.prepareStatement(
          """
            |select userid from black_list
            |""".stripMargin)
        val resultSet = statement.executeQuery()
        while (resultSet.next()) {
          val userId = resultSet.getString(1)
          blackList.append(userId)
        }
        resultSet.close()
        statement.close()
        connection.close()
        val filterRdd: RDD[AdClickData] = rdd.filter(
          row => {
            !blackList.contains(row.user)
          }
        )
        val mapRdd = filterRdd.map(
          r => {
            val format = new SimpleDateFormat("yyyyMMdd")
            val day = format.format(new Date(r.ts.toLong))
            val userId = r.user
            val adId = r.ad
            ((day, userId, adId), 1)
          }
        )
        val reduceRdd = mapRdd.reduceByKey(_ + _)
        reduceRdd
      }
    )



    wcDs.foreachRDD(
      rdd=>{
        rdd.foreachPartition(
          it=>{
            val connection = JdbcUtil.getConnection
            it.foreach{
              case ((day, userId, adId), count) => {
                println(s"===$day $userId $adId $count===")
                if(count>=30){
                  val sql = """
                              | insert into black_list values(?)
                              | on duplicate key
                              | update userid = ?
                              |""".stripMargin
                  JdbcUtil.executeUpdate(connection,sql,Array(userId,userId))
                }else{
                  val sql1 = """
                               | select
                               | *
                               | from user_ad_count where dt = ? and userid = ? and adid = ?
                               |""".stripMargin
                  val flg = JdbcUtil.isExist(connection, sql1, Array(day, userId, adId))
                  if(flg){
                    val sql2 =
                      """
                        |update user_ad_count
                        |set count = count+?
                        |where dt = ? and userid = ? and adid = ?
                        |""".stripMargin
                    JdbcUtil.executeUpdate(connection,sql2,Array(count,day,userId,adId))
                    val sql3 =
                      """
                        | select
                        | *
                        | from user_ad_count
                        | where dt = ? and userid = ? and adid = ? and count >= 30
                        |""".stripMargin
                    val flg1 = JdbcUtil.isExist(connection, sql3, Array(day, userId, adId))
                    if(flg1){
                      val connection = JdbcUtil.getConnection
                      val sql = """
                                  | insert into black_list values(?)
                                  | on duplicate key
                                  | update userid = ?
                                  |""".stripMargin
                      JdbcUtil.executeUpdate(connection,sql,Array(userId,userId))
                    }
                  }else{
                    val sql =
                      """
                        | insert into user_ad_count (dt,userid,adid,count) values(?,?,?,?)
                        |""".stripMargin
                    JdbcUtil.executeUpdate(connection,sql,Array(day,userId,adId,count))
                  }
                }
              }
            }
            connection.close()
          }
        )
      }
    )

    ssc.start()
    ssc.awaitTermination()
  }
  case class AdClickData(ts:String,area:String,city:String,user:String,ad:String)
}
```

### JDBC工具类

```scala
package com.example.spark.util

import java.sql.{Connection, PreparedStatement, ResultSet}
import java.util.Properties

import com.alibaba.druid.pool.DruidDataSourceFactory
import javax.sql.DataSource

object JdbcUtil {
  //初始化连接池
  var dataSource: DataSource = init()
  //初始化连接池方法
  def init(): DataSource = {
    val properties = new Properties()
    properties.setProperty("driverClassName", "com.mysql.jdbc.Driver")
    properties.setProperty("url", "jdbc:mysql://192.168.2.10:3306/spark")
    properties.setProperty("username", "root")
    properties.setProperty("password", "1")
    properties.setProperty("maxActive","50")
    DruidDataSourceFactory.createDataSource(properties)
  }
  //获取 MySQL 连接
  def getConnection: Connection = {
    dataSource.getConnection
  }
  //执行 SQL 语句,单条数据插入
  def executeUpdate(connection: Connection, sql: String, params: Array[Any]): Int
  = {
    var rtn = 0
    var pstmt: PreparedStatement = null
    try {
      connection.setAutoCommit(false)
      pstmt = connection.prepareStatement(sql)
      if (params != null && params.length > 0) {
        for (i <- params.indices) {
          pstmt.setObject(i + 1, params(i))
        }
      }
      rtn = pstmt.executeUpdate()
      connection.commit()
      pstmt.close()
    } catch {
      case e: Exception => e.printStackTrace()
    }
    rtn
  }
  //执行 SQL 语句,批量数据插入
  def executeBatchUpdate(connection: Connection, sql: String, paramsList:
  Iterable[Array[Any]]): Array[Int] = {
    var rtn: Array[Int] = null
    var pstmt: PreparedStatement = null
    try {
      connection.setAutoCommit(false)
      pstmt = connection.prepareStatement(sql)
      for (params <- paramsList) {
        if (params != null && params.length > 0) {
          for (i <- params.indices) {
            pstmt.setObject(i + 1, params(i))
          }
          pstmt.addBatch()
        }
      }
      rtn = pstmt.executeBatch()
      connection.commit()
      pstmt.close()
    } catch {
      case e: Exception => e.printStackTrace()
    }
    rtn
  }
  //判断一条数据是否存在
  def isExist(connection: Connection, sql: String, params: Array[Any]): Boolean =
  {
    var flag: Boolean = false
    var pstmt: PreparedStatement = null
    try {
      pstmt = connection.prepareStatement(sql)
      for (i <- params.indices) {
        pstmt.setObject(i + 1, params(i))
      }
      flag = pstmt.executeQuery().next()
      pstmt.close()
    } catch {
      case e: Exception => e.printStackTrace()
    }
    flag
  }
  //获取 MySQL 的一条数据
  def getDataFromMysql(connection: Connection, sql: String, params: Array[Any]):
  Long = {
    var result: Long = 0L
    var pstmt: PreparedStatement = null
    try {
      pstmt = connection.prepareStatement(sql)
      for (i <- params.indices) {
        pstmt.setObject(i + 1, params(i))
      }
      val resultSet: ResultSet = pstmt.executeQuery()
      while (resultSet.next()) {
        result = resultSet.getLong(1)
      }
      resultSet.close()
      pstmt.close()
    } catch {
      case e: Exception => e.printStackTrace()
    }
    result
  }
  //主方法,用于测试上述方法
  def main(args: Array[String]): Unit = {
  }
}
```