### 函数说明

将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。

```scala
def main(args: Array[String]): Unit = {
  val conf: SparkConf = new SparkConf()
    .setMaster("local[*]")
    .setAppName("Spark")
  val context = new SparkContext(conf)
  val rdd = context.makeRDD(List(1, 2, 3, 4))
  val array = rdd.map(_ * 2).collect()
  array.foreach(println)
  context.stop()
}
```

### 小功能:：从服务器日志数据 apache.log 中获取用户请求 URL 资源路径

数据样例

```log
83.149.9.216 - - 17/05/2015:10:05:03 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-search.png
83.149.9.216 - - 17/05/2015:10:05:43 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png
83.149.9.216 - - 17/05/2015:10:05:47 +0000 GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js
```

```scala
def main(args: Array[String]): Unit = {
  val conf: SparkConf = new SparkConf()
    .setMaster("local[*]")
    .setAppName("map_log")
  val sc = new SparkContext(conf)
  val rdd = sc.textFile("datas/apache.log")
  val mapRDD = rdd.map(
    line => {
      val split = line.split(" ")
      split(6)
    }
  )
  mapRDD.collect().foreach(println)
  sc.stop()
}
```

### 并行计算演示

#### 单个分区

```scala
def main(args: Array[String]): Unit = {
  val conf: SparkConf = new SparkConf()
    .setMaster("local[*]")
    .setAppName("Spark")
  val sc = new SparkContext(conf)
  val rdd = sc.makeRDD(List(1, 2, 3, 4),1)//设置一个分区
  val mapRdd = rdd.map(
    num => {
      println(Thread.currentThread().getName+">>>"+num)
      num
    }
  )
  val mapRdd1 = mapRdd.map(
    num => {
      println(Thread.currentThread().getName+"###" + num)
      num
    }
  )
  mapRdd1.collect()
  sc.stop()
}
```

结果

```log
Executor task launch worker for task 0>>>1
Executor task launch worker for task 0###1
Executor task launch worker for task 0>>>2
Executor task launch worker for task 0###2
Executor task launch worker for task 0>>>3
Executor task launch worker for task 0###3
Executor task launch worker for task 0>>>4
Executor task launch worker for task 0###4
```

#### 两个分区

```scala
def main(args: Array[String]): Unit = {
  val conf: SparkConf = new SparkConf()
    .setMaster("local[*]")
    .setAppName("Spark")
  val sc = new SparkContext(conf)
  val rdd = sc.makeRDD(List(1, 2, 3, 4),2)//设置两个分区
  val mapRdd = rdd.map(
    num => {
      println(Thread.currentThread().getName+">>>"+num)
      num
    }
  )
  val mapRdd1 = mapRdd.map(
    num => {
      println(Thread.currentThread().getName+"###" + num)
      num
    }
  )
  mapRdd1.collect()
  sc.stop()
}
```

```log
Executor task launch worker for task 1>>>3
Executor task launch worker for task 1###3
Executor task launch worker for task 1>>>4
Executor task launch worker for task 1###4
Executor task launch worker for task 0>>>1
Executor task launch worker for task 0###1
Executor task launch worker for task 0>>>2
Executor task launch worker for task 0###2
```

总结：分区内有序执行，多分区无序