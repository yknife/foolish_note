### count

返回 RDD 中元素的个数

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))
// 返回 RDD 中元素的个数
val countResult: Long = rdd.count()
```

### first

返回 RDD 中的第一个元素

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))
// 返回 RDD 中元素的个数
val firstResult: Int = rdd.first()
println(firstResult)
```

### take

返回一个由 RDD 的前 n 个元素组成的数组

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))
// 返回 RDD 中元素的个数
val takeResult: Array[Int] = rdd.take(2)
println(takeResult.mkString(","))
```

### takeOrdered

返回该 RDD 排序后的前 n 个元素组成的数组

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1,3,2,4))
// 返回 RDD 中元素的个数
val result: Array[Int] = rdd.takeOrdered(2)
```

### aggregate

分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4), 2)
// 将该 RDD 所有元素相加得到结果
//val result: Int = rdd.aggregate(0)(_ + _, _ + _)
//结果：10
// 如果初始值为10，结果为40，
//原因：与aggregateByKey有区别，aggregate分区内和分区间运算都会加上初始值
val result: Int = rdd.aggregate(10)(_ + _, _ + _)
//结果40
```

### fold

折叠操作，aggregate 的简化版操作

```scala
val rdd: RDD[Int] = sc.makeRDD(List(1, 2, 3, 4))
val foldResult: Int = rdd.fold(0)(_+_)
```

### countByKey

统计每种 key 的个数

```scala
def main(args: Array[String]): Unit = {
  val conf: SparkConf = new SparkConf()
    .setMaster("local[*]")
    .setAppName("Spark")
  val context = new SparkContext(conf)
  val rdd: RDD[(Int, String)] = context.makeRDD(List((1, "a"), (1, "a"), (1, "a"), (2,
    "b"), (3, "c"), (3, "c")))
  // 统计每种 key 的个数
  val result: collection.Map[Int, Long] = rdd.countByKey()
  println(result)
  //Map(1 -> 3, 2 -> 1, 3 -> 2)
  context.stop()
}
```

